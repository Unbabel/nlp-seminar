# Welcome to Unbabel's AI Reading Group Page

We have a weekly gathering to discuss recent papers in NLP and AI. These are very informal and relaxed reading meetings, so slides are not required although presenters usually make them.

## Schedule

### 2020-06-29

Patrick Fernandes

Paper: [\[2005.02354\] It's Easier to Translate out of English than into it: Measuring Neural Translation Difficulty by Cross-Mutual Information](https://arxiv.org/abs/2005.02354)

[Slides](https://docs.google.com/presentation/d/156GOOxK8TJZ1OVnVK9-HnxtKhiz1AIAGLX_dAwj7x1Q/edit?usp=sharing)

### 2020-06-22

Catarina F. and Ricardo

Paper: [\[2006.06264\] Tangled up in BLEU](https://arxiv.org/abs/2006.06264)

[Slides](https://drive.google.com/file/d/15Z-XlLTmQuIPcWvg83o99_NyqVg0uCxZ/view)


### 2020-06-15

Austin

Paper: [Mirror-Generative Neural Machine Translation](https://openreview.net/pdf?id=HkxQRTNYPH)

[Slides](https://docs.google.com/presentation/d/1y8Ex1t9KnQ96OZmbhKBPk-1zuj8MVN3cCuPYeXb36xU/view)


### 2020-06-08

Craig

Paper: [On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation](https://arxiv.org/abs/2005.01196)

[Slides](https://docs.google.com/presentation/d/1nxg7WiyVu2r7AH8ahMF8NmcrRVjzznA6Y-tlUcQddWM/view)


### 2020-06-01

@jose.souza

Paper: [\[2004.12681\] Lexically Constrained Neural Machine Translation with Levenshtein Transformer](https://arxiv.org/abs/2004.12681)

[Slides](https://docs.google.com/presentation/d/1yrCBwarehq06ifm6TFU7FxSZ_TQTJpGmAXy4bbcFn3I/view)

### 2020-05-25

@Ricardo

Paper: [\[2004.13637\] Recipes for building an open-domain
chatbot](https://arxiv.org/abs/2004.13637)

Slides: [retrival vs generative chatbots](https://drive.google.com/file/d/17UQPPWIdsa7MSO9LhFO76t0_o7MjhTdM/view?usp=sharing)

### 2020-05-18

@Daan and @Fabio

Paper: [\[1905.00076\] Ensemble Distribution
Distillation](https://arxiv.org/abs/1905.00076) and [\[2002.11531\] A
general framework for ensemble distribution
distillation](https://arxiv.org/abs/2002.11531)

Slides: [RG - Ensemble Distribution
Distillation](https://docs.google.com/presentation/d/1fEJgP-liWBpPd3HMNImt9eUNXpyJCatkmcH-jAfjycM/edit?usp=sharing)

### 2020-05-11

@Patrick

Paper: [\[2004.03061\] Information-Theoretic Probing for Linguistic
Structure](https://arxiv.org/abs/2004.03061)

Slides: [Information-Theoretic Probing for Linguistic
Structure](https://docs.google.com/presentation/d/1v4dFsjS78yUjytuCmQPnqYFKnSsbl5W8AL3NtIKBtjU/edit?usp=sharing)

### 2020-05-04

@Daan

Paper: [\[2003.12298\] Information-Theoretic Probing with Minimum
Description Length](https://arxiv.org/abs/2003.12298)

Blog: [Information-Theoretic Probing with
MDL](https://lena-voita.github.io/posts/mdl_probes.html)

Slides: [Information-Theoretic
Probing](https://docs.google.com/presentation/d/1lDDK2t-QWDz7UjBr0M9AX1p6y7CchHLl6ihgDB70Hik/edit?usp=sharing)

### 2020-03-30

Pedro Lobato

Paper: [ELECTRA: Pre-Training Text Encoders as Discriminators rather
than Generators](https://openreview.net/pdf?id=r1xMH1BtvB)

Slides: [Google
Slides](https://docs.google.com/presentation/d/1I1MsGSgIFx_bDJ808QmRosxpliCJmApKhBSE_et2zEw/edit?usp=sharing)

### 2020-03-09

@Katya

Paper: [Mixout: Effective Regularization to Finetune Large-scale
Pretrained Language Models](https://arxiv.org/abs/1909.11299) and
[Mix-review: Alleviate Forgetting in the Pretrain-Finetune Framework
for Neural Language Generation
Models](https://arxiv.org/abs/1910.07117)

Slides: [mixout and
mix-review](https://docs.google.com/presentation/d/1BDLiiJcu3116iiJXdgjM6w9VuuCn5d59dF5rKtl1LYU/edit?usp=sharing)

### 2020-02-17

@Daan

Paper: [Torch-Struct: Deep Structured Prediction
Library](https://arxiv.org/pdf/2002.00876.pdf)

Slides:
[Torch-Struct](https://docs.google.com/presentation/d/1mOXAodxBszFMm0AlXwQVIvf-OWRoGgaJ28dWh4wGcZo/edit?usp=sharing)

### 2020-01-27

@Nuno Miguel G and Pedro Lobato

Paper: [Reformer: The Efficient
Transformer](https://arxiv.org/abs/2001.04451)
([PDF](https://arxiv.org/pdf/2001.04451.pdf))

### 2020-01-12

@Amin and @António

Paper: 

### 2020-01-05

@Ekaterina

Paper: [Optimizing data usage via differentiable
rewards](https://arxiv.org/abs/1911.10088)

Slides: [“Optimizing data usage via differentiable rewards” by Xinyi
Wang et
al.](https://docs.google.com/presentation/d/1nb6VPWbu__hzitrseOwNXFkFEakhqjv4Ap8XerdmfgY/edit?usp=sharing)

  

### 2019-12-30

@Catarina F

Paper: [Gmail Smart Compose: Real-Time Assisted
Writing](https://arxiv.org/pdf/1906.00080.pdf)

### 2019-12-16

@Fabio and @Amin

Paper: [Neural Machine Translation with Soft
Prototype](https://papers.nips.cc/paper/8861-neural-machine-translation-with-soft-prototype.pdf)

### 2019-12-09

@Miguel and @Nuno Miguel G

Paper: [Improving Conditioning in Context-Aware Sequence to Sequence
Models](https://arxiv.org/pdf/1911.09728.pdf)

### 2019-12-02

@Ekaterina and @Fabio

Paper: [On NMT Search Errors and Model Errors: Cat Got Your
Tongue?](https://www.aclweb.org/anthology/D19-1331/)

### 2019-11-25

@António

Paper: [Exploring the Limits of Transfer Learning with a Unified
Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)

### 2019-11-18

Pedro Lobato

Paper: [Mask-Predict: Parallel Decoding of Conditional Masked Language
Models](https://arxiv.org/pdf/1904.09324.pdf)

### 2019-11-11

@Fabio

Paper: [Improving Back-Translation with Uncertainty-based Confidence
Estimation](https://arxiv.org/abs/1909.00157)

### 2019-11-04

@Sérgio

Paper: [CORRECTION OF AUTOMATIC SPEECH RECOGNITION WITH TRANSFORMER
SEQUENCE-TO-SEQUENCE MODEL](https://arxiv.org/pdf/1910.10697.pdf)

### 2019-10-28

@Ricardo

Paper: [75 Languages, 1 Model: Parsing Universal Dependencies
Universally](https://arxiv.org/abs/1904.02099)

### 2019-10-21

Rafaela Saraiva

Paper: T[raining Neural Response Selection for Task-Oriented Dialogue
Systems ](https://arxiv.org/abs/1906.01543)

### 2019-10-14

Pedro Lobato

Paper: [TinyBERT](https://arxiv.org/abs/1909.10351)

### 2019-09-09

Pedro Lobato

Paper: [Large Memory Layers with Product
Keys](https://arxiv.org/abs/1907.05242)

### 2019-08-26

Invited speaker: Patrick Fernandes

Paper: [Structured Neural
Summarization](https://openreview.net/pdf?id=H1ersoRqtm) (ICLR 2019)

### 2019-08-19

@Ricardo

Paper: [Do Neural Dialog Systems Use Conversation History Effectively?
An Empirical Study](https://arxiv.org/abs/1906.01603) (And if we have
time: [Pretraining Methods for Dialog Context Representation
Learning](https://arxiv.org/abs/1906.00414))

### 2019-08-12

ACL compilation and digest.

### 2019-08-05

@Daan

Title: Neural language models with latent syntax

Description: In this talk I will present my thesis work at the
University of Amsterdam under supervision of Wilker Aziz. In the work I
investigate semi-supervised and unsupervised learning of the recurrent
neural network grammar (RNNG) (Dyer et al. 2016). I will also briefly
describe concurrent work by Kim et al. (2019) who (unbeknownst to me)
worked on an almost identical approach.

Slides:
[https://github.com/daandouwe/thesis/blob/master/doc/presentation-unbabel.pdf](https://github.com/daandouwe/thesis/blob/master/doc/presentation-unbabel.pdf)

Links:

  - Thesis
    [https://msclogic.illc.uva.nl/theses/archive/publication/4811/Neural-language-models-with-latent-syntax](https://msclogic.illc.uva.nl/theses/archive/publication/4811/Neural-language-models-with-latent-syntax)
    and code
    [https://github.com/daandouwe/thesis](https://github.com/daandouwe/thesis)
  - Original RNNG (Dyer et al. 2016)
    [https://www.aclweb.org/anthology/N16-1024](https://www.aclweb.org/anthology/N16-1024)
  - Unsupervised RNNG (Kim et al. 2019)
    [https://www.aclweb.org/anthology/N19-1114](https://www.aclweb.org/anthology/N19-1114)

### 2019-07-22

Invited speaker: Daniel Loureiro

He’s going to talk about his accepted paper at ACL:

“Language Modelling Makes Sense: Propagating Representations through
WordNet for Full-Coverage Word Sense Disambiguation”

### 2019-07-08

Antonio Gois

Resuming XLnet and related papers

Slides: [non-sequential
overview](https://docs.google.com/presentation/d/16-dtI3Xjnb49ZB91GcP22yrfUKB8zSihE31PMWsANPs/edit?usp=sharing)

### 2019-07-01

@Tsvetomila and @Marcos

Paper: [\[1906.08237\] XLNet: Generalized Autoregressive Pretraining
for Language Understanding](https://arxiv.org/abs/1906.08237)

Slides:
[XLNet](https://docs.google.com/presentation/d/1XiQ-MaPiA656l1WwPaws4w4jFuKXD4TxIAvZ5rdlyJ4/edit?usp=sharing)

### 2019-06-24

@Amin and @António

2nd part

### 2019-06-17

@Amin and @António

Papers: TBD
